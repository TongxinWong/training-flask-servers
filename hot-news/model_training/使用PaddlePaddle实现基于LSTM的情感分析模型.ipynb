{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: Tongxin Wong Based on code from paddlepaddle\n",
    "create time: 2020-07-23\n",
    "update time: 2020-07-25\n",
    "\"\"\"\n",
    "# 加载所需用到的包\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import six\n",
    "import requests\n",
    "import string\n",
    "import tarfile\n",
    "import hashlib\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "from paddle.fluid.dygraph.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0, 很快，好吃，味道足，量大\n",
      "sentence 0, label 1\n",
      "sentence 1, 没有送水没有送水没有送水\n",
      "sentence 1, label 1\n",
      "sentence 2, 非常快，态度好。\n",
      "sentence 2, label 1\n",
      "sentence 3, 方便，快捷，味道可口，快递给力\n",
      "sentence 3, label 1\n",
      "sentence 4, 菜味道很棒！送餐很及时！\n",
      "sentence 4, label 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 加载数据集\n",
    "def load_data(ratio):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    # 读取正面和负面评论的csv文件，正面label = 1，负面label = 0\n",
    "    # 外卖评价数据\n",
    "    csv_data = pd.read_csv('./dataset/waimai_10k.csv')\n",
    "    # 按比例划分训练集和测试集\n",
    "    # 将数据插入列表\n",
    "    for i in range(int(4000 * ratio)):\n",
    "        item = csv_data[i:i+1]\n",
    "        sentence = item['review'][i]\n",
    "        label = item['label'][i]\n",
    "        train_set.append((sentence, label))\n",
    "        \n",
    "    for i in range(int(4000 * ratio), 4000):\n",
    "        item = csv_data[i:i+1]\n",
    "        sentence = item['review'][i]\n",
    "        label = item['label'][i]\n",
    "        test_set.append((sentence, label))\n",
    "        \n",
    "    for i in range(4000, 4000 + int(7987 * ratio)):\n",
    "        item = csv_data[i:i+1]\n",
    "        sentence = item['review'][i]\n",
    "        label = item['label'][i]\n",
    "        train_set.append((sentence, label))\n",
    "        \n",
    "    for i in range(4000 + int(7987 * ratio), csv_data.shape[0]):\n",
    "        item = csv_data[i:i+1]\n",
    "        sentence = item['review'][i]\n",
    "        label = item['label'][i]\n",
    "        test_set.append((sentence, label))\n",
    "        \n",
    "#     csv_data = pd.read_csv('./dataset/ChnSentiCorp_htl_all.csv')\n",
    "#     # 按比例划分训练集和测试集\n",
    "#     # 将数据插入列表\n",
    "#     for i in range(int(5322 * ratio)):\n",
    "#         item = csv_data[i:i+1]\n",
    "#         sentence = item['review'][i]\n",
    "#         label = item['label'][i]\n",
    "#         train_set.append((sentence, label))\n",
    "        \n",
    "#     for i in range(int(5322 * ratio), 5322):\n",
    "#         item = csv_data[i:i+1]\n",
    "#         sentence = item['review'][i]\n",
    "#         label = item['label'][i]\n",
    "#         test_set.append((sentence, label))\n",
    "        \n",
    "#     for i in range(5322, 5322 + int(2444 * ratio)):\n",
    "#         item = csv_data[i:i+1]\n",
    "#         sentence = item['review'][i]\n",
    "#         label = item['label'][i]\n",
    "#         train_set.append((sentence, label))\n",
    "        \n",
    "#     for i in range(5322 + int(2444 * ratio), csv_data.shape[0]):\n",
    "#         item = csv_data[i:i+1]\n",
    "#         sentence = item['review'][i]\n",
    "#         label = item['label'][i]\n",
    "#         test_set.append((sentence, label))\n",
    "        \n",
    "    return train_set, test_set\n",
    "        \n",
    "# 将前80%的数据划为训练集\n",
    "train_corpus, test_corpus = load_data(0.8)\n",
    "\n",
    "# 打印数据查看\n",
    "for i in range(5):\n",
    "    print(\"sentence %d, %s\" % (i, train_corpus[i][0]))    \n",
    "    print(\"sentence %d, label %d\" % (i, train_corpus[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "2020-07-25 16:25:34,092-DEBUG: Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "# 对语料进行切词，使用jieba\n",
    "import jieba\n",
    "# 启动paddle模式\n",
    "jieba.enable_paddle()\n",
    "\n",
    "# 创建停用词list\n",
    "# def get_stopwords_list(file_path):\n",
    "#     stopwords = [line.strip() for line in open(file_path, 'r', encoding='utf-8').readlines()]\n",
    "#     return stopwords\n",
    "\n",
    "def data_preprocess(corpus):\n",
    "    data_set = []\n",
    "    # 停用词列表\n",
    "    # stopwords = get_stopwords_list('./dataset/baidu_stopwords.txt')\n",
    "    for sentence, label in corpus:\n",
    "        sentence = str(sentence)\n",
    "        words = jieba.cut(sentence, use_paddle=True)\n",
    "        # 去除停用词\n",
    "        # words = list(set(list(words)).difference(set(stopwords)))\n",
    "        data_set.append((list(words),label))\n",
    "        \n",
    "    return data_set\n",
    "\n",
    "train_corpus = data_preprocess(train_corpus)\n",
    "test_corpus = data_preprocess(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 11463 different words in the corpus\n",
      "word [oov], its id 0, its word freq 10000000000\n",
      "word [pad], its id 1, its word freq 10000000000\n",
      "word ，, its id 2, its word freq 10123\n",
      "word 了, its id 3, its word freq 7556\n",
      "word 的, its id 4, its word freq 6819\n",
      "word ,, its id 5, its word freq 3506\n",
      "word 。, its id 6, its word freq 2603\n",
      "word ！, its id 7, its word freq 2131\n",
      "word 我, its id 8, its word freq 1775\n",
      "word 吃, its id 9, its word freq 1715\n",
      "word 也, its id 10, its word freq 1554\n",
      "word 是, its id 11, its word freq 1539\n",
      "word 不, its id 12, its word freq 1524\n",
      "word 都, its id 13, its word freq 1514\n",
      "word 太, its id 14, its word freq 1423\n",
      "word 味道, its id 15, its word freq 1282\n",
      "word 送, its id 16, its word freq 1259\n",
      "word 送餐, its id 17, its word freq 1222\n",
      "word 没, its id 18, its word freq 1183\n",
      "word 还, its id 19, its word freq 1092\n",
      "word 慢, its id 20, its word freq 975\n",
      "word 给, its id 21, its word freq 966\n",
      "word 就, its id 22, its word freq 962\n",
      "word 没有, its id 23, its word freq 915\n",
      "word 很, its id 24, its word freq 859\n",
      "word 好吃, its id 25, its word freq 791\n",
      "word 好, its id 26, its word freq 739\n",
      "word 点, its id 27, its word freq 708\n",
      "word 不错, its id 28, its word freq 698\n",
      "word 说, its id 29, its word freq 668\n",
      "word 送到, its id 30, its word freq 662\n",
      "word 到, its id 31, its word freq 651\n",
      "word 才, its id 32, its word freq 582\n",
      "word 就是, its id 33, its word freq 566\n",
      "word 快, its id 34, its word freq 541\n",
      "word 少, its id 35, its word freq 540\n",
      "word 速度, its id 36, its word freq 535\n",
      "word 等, its id 37, its word freq 526\n",
      "word ., its id 38, its word freq 521\n",
      "word 凉, its id 39, its word freq 503\n",
      "word 啊, its id 40, its word freq 493\n",
      "word 要, its id 41, its word freq 492\n",
      "word ？, its id 42, its word freq 480\n",
      "word 菜, its id 43, its word freq 469\n",
      "word 很好, its id 44, its word freq 462\n",
      "word 再, its id 45, its word freq 457\n",
      "word 送来, its id 46, its word freq 451\n",
      "word 在, its id 47, its word freq 446\n",
      "word 还是, its id 48, its word freq 435\n",
      "word 有, its id 49, its word freq 434\n"
     ]
    }
   ],
   "source": [
    "#构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    word_freq_dict = dict()\n",
    "    for sentence, _ in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = 0\n",
    "            word_freq_dict[word] += 1\n",
    "\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "\n",
    "    #一般来说，我们把oov和pad放在词典前面，给他们一个比较小的id，这样比较方便记忆，并且易于后续扩展词表\n",
    "    word2id_dict['[oov]'] = 0\n",
    "    word2id_freq[0] = 1e10\n",
    "\n",
    "    word2id_dict['[pad]'] = 1\n",
    "    word2id_freq[1] = 1e10\n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        word2id_dict[word] = len(word2id_dict)\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "\n",
    "    return word2id_freq, word2id_dict\n",
    "\n",
    "word2id_freq, word2id_dict = build_dict(train_corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9589 tokens in the corpus\n",
      "[([54, 2, 25, 2, 15, 299, 289, 82], 1), ([23, 16, 405, 23, 16, 405, 23, 16, 405], 1), ([60, 399, 75, 26, 6], 1), ([564, 2, 2181, 2, 15, 3290, 126, 308], 1), ([43, 15, 24, 256, 7, 17, 24, 319, 7], 1)]\n",
      "[([173, 4, 2, 54, 6], 1), ([1171, 33, 0, 13, 55, 108], 1), ([28, 2, 33, 2881, 13, 55, 108], 1), ([36, 810, 20, 3], 1), ([1421, 0, 444, 293, 1452, 30, 0, 48, 0, 4, 2, 44, 2, 0, 2, 254, 457, 3723], 1)]\n"
     ]
    }
   ],
   "source": [
    "#把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        #将句子中的词逐个替换成id，如果句子中的词不在词表内，则替换成oov\n",
    "        #这里需要注意，一般来说我们可能需要查看一下test-set中，句子oov的比例，\n",
    "        #如果存在过多oov的情况，那就说明我们的训练数据不足或者切分存在巨大偏差，需要调整\n",
    "        sentence = [word2id_dict[word] if word in word2id_dict \\\n",
    "                    else word2id_dict['[oov]'] for word in sentence]    \n",
    "        data_set.append((sentence, sentence_label))\n",
    "    return data_set\n",
    "\n",
    "train_corpus = convert_corpus_to_id(train_corpus, word2id_dict)\n",
    "test_corpus = convert_corpus_to_id(test_corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[  25],\n",
      "        [   2],\n",
      "        [  17],\n",
      "        [  20],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[1705],\n",
      "        [  28],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 344],\n",
      "        [  21],\n",
      "        [ 204],\n",
      "        [ 246],\n",
      "        [  57],\n",
      "        [ 189],\n",
      "        [ 101],\n",
      "        [   4],\n",
      "        [  57],\n",
      "        [  21],\n",
      "        [ 101],\n",
      "        [   6],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[1],\n",
      "       [0],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[  37],\n",
      "        [ 223],\n",
      "        [   2],\n",
      "        [1296],\n",
      "        [  67],\n",
      "        [   4],\n",
      "        [ 191],\n",
      "        [ 560],\n",
      "        [1260],\n",
      "        [  32],\n",
      "        [  30],\n",
      "        [ 559],\n",
      "        [ 285],\n",
      "        [  22],\n",
      "        [1364],\n",
      "        [2907],\n",
      "        [   4],\n",
      "        [1387],\n",
      "        [  40],\n",
      "        [   2],\n",
      "        [  20],\n",
      "        [ 237],\n",
      "        [   3],\n",
      "        [   7],\n",
      "        [ 301],\n",
      "        [ 396],\n",
      "        [  91],\n",
      "        [  11],\n",
      "        [  17],\n",
      "        [  20]],\n",
      "\n",
      "       [[3550],\n",
      "        [  62],\n",
      "        [  57],\n",
      "        [   2],\n",
      "        [ 461],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 423],\n",
      "        [   2],\n",
      "        [  75],\n",
      "        [  76],\n",
      "        [2737],\n",
      "        [   2],\n",
      "        [1541],\n",
      "        [  14],\n",
      "        [1166],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[0],\n",
      "       [1],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[  59],\n",
      "        [  11],\n",
      "        [   8],\n",
      "        [   9],\n",
      "        [ 121],\n",
      "        [ 632],\n",
      "        [   4],\n",
      "        [ 169],\n",
      "        [  61],\n",
      "        [   5],\n",
      "        [ 538],\n",
      "        [  38],\n",
      "        [2899],\n",
      "        [ 112],\n",
      "        [1728],\n",
      "        [ 155],\n",
      "        [1814],\n",
      "        [   4],\n",
      "        [ 833],\n",
      "        [1815],\n",
      "        [ 935],\n",
      "        [ 167],\n",
      "        [   5],\n",
      "        [ 578],\n",
      "        [  38],\n",
      "        [6125],\n",
      "        [ 112],\n",
      "        [ 737],\n",
      "        [ 246],\n",
      "        [ 155]],\n",
      "\n",
      "       [[ 127],\n",
      "        [  81],\n",
      "        [   2],\n",
      "        [ 341],\n",
      "        [  67],\n",
      "        [1849],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 193],\n",
      "        [  27],\n",
      "        [ 131],\n",
      "        [ 103],\n",
      "        [   4],\n",
      "        [ 222],\n",
      "        [   2],\n",
      "        [ 166],\n",
      "        [6012],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [1400],\n",
      "        [ 269],\n",
      "        [1803],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [ 186],\n",
      "        [ 158],\n",
      "        [  35],\n",
      "        [ 116],\n",
      "        [  27],\n",
      "        [1604],\n",
      "        [ 172],\n",
      "        [ 792],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[0],\n",
      "       [0],\n",
      "       [1]], dtype=int64))\n",
      "(array([[[2017],\n",
      "        [ 748],\n",
      "        [ 120],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [ 717],\n",
      "        [2017],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[  15],\n",
      "        [  53],\n",
      "        [   6],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[   9],\n",
      "        [ 215],\n",
      "        [1437],\n",
      "        [2728],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[1],\n",
      "       [0],\n",
      "       [1]], dtype=int64))\n",
      "(array([[[  44],\n",
      "        [   2],\n",
      "        [ 347],\n",
      "        [ 796],\n",
      "        [  25],\n",
      "        [   2],\n",
      "        [  97],\n",
      "        [1976],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[  15],\n",
      "        [ 156],\n",
      "        [ 869],\n",
      "        [   5],\n",
      "        [  59],\n",
      "        [ 131],\n",
      "        [ 427],\n",
      "        [   4],\n",
      "        [  11],\n",
      "        [  77],\n",
      "        [   9],\n",
      "        [   4],\n",
      "        [  99],\n",
      "        [ 151],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 469],\n",
      "        [   2],\n",
      "        [1445],\n",
      "        [  62],\n",
      "        [  78],\n",
      "        [2427],\n",
      "        [ 148],\n",
      "        [ 264],\n",
      "        [   9],\n",
      "        [   7],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[1],\n",
      "       [0],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[7255],\n",
      "        [ 428],\n",
      "        [4052],\n",
      "        [   2],\n",
      "        [ 939],\n",
      "        [   3],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[7783],\n",
      "        [  24],\n",
      "        [  81],\n",
      "        [7784],\n",
      "        [  28],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 114],\n",
      "        [   3],\n",
      "        [  60],\n",
      "        [ 358],\n",
      "        [  22],\n",
      "        [  12],\n",
      "        [  29],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [  59],\n",
      "        [ 376],\n",
      "        [ 181],\n",
      "        [ 292],\n",
      "        [ 821],\n",
      "        [  12],\n",
      "        [1284],\n",
      "        [ 361],\n",
      "        [   4],\n",
      "        [ 501],\n",
      "        [   5],\n",
      "        [  72],\n",
      "        [ 826],\n",
      "        [ 112],\n",
      "        [ 246],\n",
      "        [  68],\n",
      "        [1284],\n",
      "        [ 361],\n",
      "        [  40],\n",
      "        [  42],\n",
      "        [ 128]]], dtype=int64), array([[0],\n",
      "       [0],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[ 549],\n",
      "        [   9],\n",
      "        [ 910],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 229],\n",
      "        [  43],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [7365],\n",
      "        [ 122],\n",
      "        [  19],\n",
      "        [3800],\n",
      "        [  68],\n",
      "        [ 576],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 135],\n",
      "        [  47],\n",
      "        [ 820],\n",
      "        [  22],\n",
      "        [ 327],\n",
      "        [  31],\n",
      "        [   4],\n",
      "        [   2],\n",
      "        [ 286],\n",
      "        [  13],\n",
      "        [ 448],\n",
      "        [   3],\n",
      "        [ 432],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [   8],\n",
      "        [ 566],\n",
      "        [  19],\n",
      "        [  12],\n",
      "        [ 852],\n",
      "        [  27],\n",
      "        [ 316],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [ 163],\n",
      "        [ 135],\n",
      "        [ 446],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [ 432]]], dtype=int64), array([[1],\n",
      "       [0],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[  295],\n",
      "        [    4],\n",
      "        [   15],\n",
      "        [  182],\n",
      "        [  601],\n",
      "        [    4],\n",
      "        [  114],\n",
      "        [    3],\n",
      "        [ 1452],\n",
      "        [ 2443],\n",
      "        [  109],\n",
      "        [    3],\n",
      "        [   12],\n",
      "        [   29],\n",
      "        [    2],\n",
      "        [   17],\n",
      "        [    4],\n",
      "        [   77],\n",
      "        [   19],\n",
      "        [11329],\n",
      "        [11330],\n",
      "        [    4],\n",
      "        [    2],\n",
      "        [   59],\n",
      "        [ 1846],\n",
      "        [   56],\n",
      "        [    9],\n",
      "        [    4],\n",
      "        [   24],\n",
      "        [ 4214]],\n",
      "\n",
      "       [[ 1800],\n",
      "        [  152],\n",
      "        [   55],\n",
      "        [    9],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]],\n",
      "\n",
      "       [[  172],\n",
      "        [   11],\n",
      "        [   14],\n",
      "        [   20],\n",
      "        [    3],\n",
      "        [    2],\n",
      "        [   37],\n",
      "        [    3],\n",
      "        [  220],\n",
      "        [ 1234],\n",
      "        [   13],\n",
      "        [   39],\n",
      "        [    3],\n",
      "        [ 4392],\n",
      "        [   39],\n",
      "        [    3],\n",
      "        [  444],\n",
      "        [    9],\n",
      "        [   92],\n",
      "        [   42],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]]], dtype=int64), array([[0],\n",
      "       [1],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[ 180],\n",
      "        [7922],\n",
      "        [ 581],\n",
      "        [ 145],\n",
      "        [   3],\n",
      "        [   2],\n",
      "        [ 194],\n",
      "        [ 735],\n",
      "        [  78],\n",
      "        [ 208],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[  16],\n",
      "        [   4],\n",
      "        [  24],\n",
      "        [ 114],\n",
      "        [   5],\n",
      "        [  15],\n",
      "        [  53],\n",
      "        [4653],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]],\n",
      "\n",
      "       [[ 512],\n",
      "        [ 230],\n",
      "        [  20],\n",
      "        [   4],\n",
      "        [ 136],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1],\n",
      "        [   1]]], dtype=int64), array([[0],\n",
      "       [0],\n",
      "       [0]], dtype=int64))\n",
      "(array([[[   43],\n",
      "        [  173],\n",
      "        [    2],\n",
      "        [  101],\n",
      "        [   49],\n",
      "        [  799],\n",
      "        [ 8422],\n",
      "        [  221],\n",
      "        [ 2807],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]],\n",
      "\n",
      "       [[ 3989],\n",
      "        [  684],\n",
      "        [ 1201],\n",
      "        [    2],\n",
      "        [ 1995],\n",
      "        [   67],\n",
      "        [    4],\n",
      "        [  191],\n",
      "        [11419],\n",
      "        [   32],\n",
      "        [   30],\n",
      "        [    7],\n",
      "        [   89],\n",
      "        [  225],\n",
      "        [   65],\n",
      "        [    7],\n",
      "        [   45],\n",
      "        [   10],\n",
      "        [ 1362],\n",
      "        [  434],\n",
      "        [  103],\n",
      "        [   67],\n",
      "        [    3],\n",
      "        [    7],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]],\n",
      "\n",
      "       [[  249],\n",
      "        [    2],\n",
      "        [   36],\n",
      "        [   10],\n",
      "        [   54],\n",
      "        [    6],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1],\n",
      "        [    1]]], dtype=int64), array([[0],\n",
      "       [0],\n",
      "       [1]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#编写一个迭代器，每次调用这个迭代器都会返回一个新的batch，用于训练或者预测\n",
    "def build_batch(word2id_dict, corpus, batch_size, epoch_num, max_seq_len, shuffle = True):\n",
    "\n",
    "    #模型将会接受的两个输入：\n",
    "    # 1. 一个形状为[batch_size, max_seq_len]的张量，sentence_batch，代表了一个mini-batch的句子。\n",
    "    # 2. 一个形状为[batch_size, 1]的张量，sentence_label_batch，\n",
    "    #    每个元素都是非0即1，代表了每个句子的情感类别（正向或者负向）\n",
    "    sentence_batch = []\n",
    "    sentence_label_batch = []\n",
    "\n",
    "    for _ in range(epoch_num): \n",
    "\n",
    "        #每个epcoh前都shuffle一下数据，有助于提高模型训练的效果\n",
    "        #但是对于预测任务，不要做数据shuffle\n",
    "        if shuffle:\n",
    "            random.shuffle(corpus)\n",
    "\n",
    "        for sentence, sentence_label in corpus:\n",
    "            sentence_sample = sentence[:min(max_seq_len, len(sentence))]\n",
    "            if len(sentence_sample) < max_seq_len:\n",
    "                for _ in range(max_seq_len - len(sentence_sample)):\n",
    "                    sentence_sample.append(word2id_dict['[pad]'])\n",
    "            \n",
    "            \n",
    "            sentence_sample = [[word_id] for word_id in sentence_sample]\n",
    "\n",
    "            sentence_batch.append(sentence_sample)\n",
    "            sentence_label_batch.append([sentence_label])\n",
    "\n",
    "            if len(sentence_batch) == batch_size:\n",
    "                yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "                sentence_batch = []\n",
    "                sentence_label_batch = []\n",
    "\n",
    "    if len(sentence_batch) == batch_size:\n",
    "        yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "\n",
    "for _, batch in zip(range(10), build_batch(word2id_dict, \n",
    "                    train_corpus, batch_size=3, epoch_num=3, max_seq_len=30)):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\n",
    "#使用飞桨实现一个长短时记忆模型\n",
    "class SimpleLSTMRNN(fluid.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_steps,\n",
    "                 num_layers=1,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "        \n",
    "        #这个模型有几个参数：\n",
    "        #1. hidden_size，表示embedding-size，或者是记忆向量的维度\n",
    "        #2. num_steps，表示这个长短时记忆网络，最多可以考虑多长的时间序列\n",
    "        #3. num_layers，表示这个长短时记忆网络内部有多少层，我们知道，\n",
    "        # 给定一个形状为[batch_size, seq_len, embedding_size]的输入，\n",
    "        # 长短时记忆网络会输出一个同样为[batch_size, seq_len, embedding_size]的输出，\n",
    "        # 我们可以把这个输出再链到一个新的长短时记忆网络上\n",
    "        # 如此叠加多层长短时记忆网络，有助于学习更复杂的句子甚至是篇章。\n",
    "        #4. init_scale，表示网络内部的参数的初始化范围，\n",
    "        # 长短时记忆网络内部用了很多tanh，sigmoid等激活函数，这些函数对数值精度非常敏感，\n",
    "        # 因此我们一般只使用比较小的初始化范围，以保证效果，\n",
    "        \n",
    "        super(SimpleLSTMRNN, self).__init__()\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._init_scale = init_scale\n",
    "        self._dropout = dropout\n",
    "        self._input = None\n",
    "        self._num_steps = num_steps\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "\n",
    "        # weight_1_arr用于存储不同层的长短时记忆网络中，不同门的W参数\n",
    "        self.weight_1_arr = []\n",
    "        self.weight_2_arr = []\n",
    "        # bias_arr用于存储不同层的长短时记忆网络中，不同门的b参数\n",
    "        self.bias_arr = []\n",
    "        self.mask_array = []\n",
    "\n",
    "        # 通过使用create_parameter函数，创建不同长短时记忆网络层中的参数\n",
    "        # 通过上面的公式，我们知道，我们总共需要8个形状为[_hidden_size, _hidden_size]的W向量\n",
    "        # 和4个形状为[_hidden_size]的b向量，因此，我们在声明参数的时候，\n",
    "        # 一次性声明一个大小为[self._hidden_size * 2, self._hidden_size * 4]的参数\n",
    "        # 和一个 大小为[self._hidden_size * 4]的参数，这样做的好处是，\n",
    "        # 可以使用一次矩阵计算，同时计算8个不同的矩阵乘法\n",
    "        # 以便加快计算速度\n",
    "        for i in range(self._num_layers):\n",
    "            weight_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.UniformInitializer(\n",
    "                        low=-self._init_scale, high=self._init_scale)),\n",
    "                shape=[self._hidden_size * 2, self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-self._init_scale, high=self._init_scale))\n",
    "            self.weight_1_arr.append(self.add_parameter('w_%d' % i, weight_1))\n",
    "            bias_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.UniformInitializer(\n",
    "                        low=-self._init_scale, high=self._init_scale)),\n",
    "                shape=[self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.Constant(0.0))\n",
    "            self.bias_arr.append(self.add_parameter('b_%d' % i, bias_1))\n",
    "\n",
    "    # 定义LSTM网络的前向计算逻辑，飞桨会自动根据前向计算结果，给出反向结果\n",
    "    def forward(self, input_embedding, init_hidden=None, init_cell=None):\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "        \n",
    "        #输入有三个信号：\n",
    "        # 1. input_embedding，这个就是输入句子的embedding表示，\n",
    "        # 是一个形状为[batch_size, seq_len, embedding_size]的张量\n",
    "        # 2. init_hidden，这个表示LSTM中每一层的初始h的值，有时候，\n",
    "        # 我们需要显示地指定这个值，在不需要的时候，就可以把这个值设置为空\n",
    "        # 3. init_cell，这个表示LSTM中每一层的初始c的值，有时候，\n",
    "        # 我们需要显示地指定这个值，在不需要的时候，就可以把这个值设置为空\n",
    "\n",
    "        # 我们需要通过slice操作，把每一层的初始hidden和cell值拿出来，\n",
    "        # 并存储在cell_array和hidden_array中\n",
    "        for i in range(self._num_layers):\n",
    "            pre_hidden = fluid.layers.slice(\n",
    "                init_hidden, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_cell = fluid.layers.slice(\n",
    "                init_cell, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_hidden = fluid.layers.reshape(\n",
    "                pre_hidden, shape=[-1, self._hidden_size])\n",
    "            pre_cell = fluid.layers.reshape(\n",
    "                pre_cell, shape=[-1, self._hidden_size])\n",
    "            self.hidden_array.append(pre_hidden)\n",
    "            self.cell_array.append(pre_cell)\n",
    "\n",
    "        # res记录了LSTM中每一层的输出结果（hidden）\n",
    "        res = []\n",
    "        for index in range(self._num_steps):\n",
    "            # 首先需要通过slice函数，拿到输入tensor input_embedding中当前位置的词的向量表示\n",
    "            # 并把这个词的向量表示转换为一个大小为 [batch_size, embedding_size]的张量\n",
    "            self._input = fluid.layers.slice(\n",
    "                input_embedding, axes=[1], starts=[index], ends=[index + 1])\n",
    "            self._input = fluid.layers.reshape(\n",
    "                self._input, shape=[-1, self._hidden_size])\n",
    "            \n",
    "            # 计算每一层的结果，从下而上\n",
    "            for k in range(self._num_layers):\n",
    "                # 首先获取每一层LSTM对应上一个时间步的hidden，cell，以及当前层的W和b参数\n",
    "                pre_hidden = self.hidden_array[k]\n",
    "                pre_cell = self.cell_array[k]\n",
    "                weight_1 = self.weight_1_arr[k]\n",
    "                bias = self.bias_arr[k]\n",
    "\n",
    "                # 我们把hidden和拿到的当前步的input拼接在一起，便于后续计算\n",
    "                nn = fluid.layers.concat([self._input, pre_hidden], 1)\n",
    "                \n",
    "                # 将输入门，遗忘门，输出门等对应的W参数，和输入input和pre-hidden相乘\n",
    "                # 我们通过一步计算，就同时完成了8个不同的矩阵运算，提高了运算效率\n",
    "                gate_input = fluid.layers.matmul(x=nn, y=weight_1)\n",
    "\n",
    "                # 将b参数也加入到前面的运算结果中\n",
    "                gate_input = fluid.layers.elementwise_add(gate_input, bias)\n",
    "                \n",
    "                # 通过split函数，将每个门得到的结果拿出来\n",
    "                i, j, f, o = fluid.layers.split(\n",
    "                    gate_input, num_or_sections=4, dim=-1)\n",
    "                \n",
    "                # 把输入门，遗忘门，输出门等对应的权重作用在当前输入input和pre-hidden上\n",
    "                c = pre_cell * fluid.layers.sigmoid(f) + fluid.layers.sigmoid(\n",
    "                    i) * fluid.layers.tanh(j)\n",
    "                m = fluid.layers.tanh(c) * fluid.layers.sigmoid(o)\n",
    "                \n",
    "                # 记录当前步骤的计算结果，\n",
    "                # m是当前步骤需要输出的hidden\n",
    "                # c是当前步骤需要输出的cell\n",
    "                self.hidden_array[k] = m\n",
    "                self.cell_array[k] = c\n",
    "                self._input = m\n",
    "                \n",
    "                # 一般来说，我们有时候会在LSTM的结果结果内加入dropout操作\n",
    "                # 这样会提高模型的训练鲁棒性\n",
    "                if self._dropout is not None and self._dropout > 0.0:\n",
    "                    self._input = fluid.layers.dropout(\n",
    "                        self._input,\n",
    "                        dropout_prob=self._dropout,\n",
    "                        dropout_implementation='upscale_in_train')\n",
    "                    \n",
    "            res.append(\n",
    "                fluid.layers.reshape(\n",
    "                    self._input, shape=[1, -1, self._hidden_size]))\n",
    "        \n",
    "        # 计算长短时记忆网络的结果返回回来，包括：\n",
    "        # 1. real_res：每个时间步上不同层的hidden结果\n",
    "        # 2. last_hidden：最后一个时间步中，每一层的hidden的结果，\n",
    "        # 形状为：[batch_size, num_layers, hidden_size]\n",
    "        # 3. last_cell：最后一个时间步中，每一层的cell的结果，\n",
    "        # 形状为：[batch_size, num_layers, hidden_size]\n",
    "        real_res = fluid.layers.concat(res, 0)\n",
    "        real_res = fluid.layers.transpose(x=real_res, perm=[1, 0, 2])\n",
    "        last_hidden = fluid.layers.concat(self.hidden_array, 1)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_hidden = fluid.layers.transpose(x=last_hidden, perm=[1, 0, 2])\n",
    "        last_cell = fluid.layers.concat(self.cell_array, 1)\n",
    "        last_cell = fluid.layers.reshape(\n",
    "            last_cell, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_cell = fluid.layers.transpose(x=last_cell, perm=[1, 0, 2])\n",
    "        \n",
    "        return real_res, last_hidden, last_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\n",
    "# 定义一个可以用于情感分类的网络\n",
    "class SentimentClassifier(fluid.Layer):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 vocab_size,\n",
    "                 class_num=2,\n",
    "                 num_layers=1,\n",
    "                 num_steps=128,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "        \n",
    "        #这个模型的参数分别为：\n",
    "        #1. hidden_size，表示embedding-size，hidden和cell向量的维度\n",
    "        #2. vocab_size，模型可以考虑的词表大小\n",
    "        #3. class_num，情感类型个数，可以是2分类，也可以是多分类\n",
    "        #4. num_steps，表示这个情感分析模型最大可以考虑的句子长度\n",
    "        #5. init_scale，表示网络内部的参数的初始化范围，\n",
    "        # 长短时记忆网络内部用了很多tanh，sigmoid等激活函数，这些函数对数值精度非常敏感，\n",
    "        # 因此我们一般只使用比较小的初始化范围，以保证效果\n",
    "        \n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.init_scale = init_scale\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 声明一个LSTM模型，用来把一个句子抽象城一个向量\n",
    "        self.simple_lstm_rnn = SimpleLSTMRNN(\n",
    "            hidden_size,\n",
    "            num_steps,\n",
    "            num_layers=num_layers,\n",
    "            init_scale=init_scale,\n",
    "            dropout=dropout)\n",
    "        \n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\n",
    "        self.embedding = Embedding(\n",
    "            size=[vocab_size, hidden_size],\n",
    "            dtype='float32',\n",
    "            is_sparse=False,\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-init_scale, high=init_scale)))\n",
    "        \n",
    "        # 在得到一个句子的向量表示后，我们需要根据这个向量表示对这个句子进行分类\n",
    "        # 一般来说，我们可以把这个句子的向量表示，\n",
    "        # 乘以一个大小为[self.hidden_size, self.class_num]的W参数\n",
    "        # 并加上一个大小为[self.class_num]的b参数\n",
    "        # 通过这种手段达到把句子向量映射到分类结果的目标\n",
    "        \n",
    "        # 我们需要声明最终在使用句子向量映射到具体情感类别过程中所需要使用的参数\n",
    "        # 这个参数的大小一般是[self.hidden_size, self.class_num]\n",
    "        self.softmax_weight = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.hidden_size, self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "        # 同样的，我们需要声明最终分类过程中的b参数\n",
    "        #  这个参数的大小一般是[self.class_num]\n",
    "        self.softmax_bias = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "\n",
    "    def forward(self, input, label):\n",
    "\n",
    "        # 首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\n",
    "        init_hidden_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量\n",
    "        # 并设置stop-gradient=True，避免这些向量被更新，从而影响训练效果\n",
    "        init_hidden = fluid.dygraph.to_variable(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = fluid.dygraph.to_variable(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        init_h = fluid.layers.reshape(\n",
    "            init_hidden, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        init_c = fluid.layers.reshape(\n",
    "            init_cell, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        # 将输入的句子的mini-batch input，转换为词向量表示\n",
    "        x_emb = self.embedding(input)\n",
    "\n",
    "        x_emb = fluid.layers.reshape(\n",
    "            x_emb, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        if self.dropout is not None and self.dropout > 0.0:\n",
    "            x_emb = fluid.layers.dropout(\n",
    "                x_emb,\n",
    "                dropout_prob=self.dropout,\n",
    "                dropout_implementation='upscale_in_train')\n",
    "        \n",
    "        # 使用LSTM网络，把每个句子转换为向量表示\n",
    "        rnn_out, last_hidden, last_cell = self.simple_lstm_rnn(x_emb, init_h,\n",
    "                                                               init_c)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self.hidden_size])\n",
    "        \n",
    "        # 将每个句子的向量表示，通过矩阵计算，映射到具体的情感类别上\n",
    "        projection = fluid.layers.matmul(last_hidden, self.softmax_weight)\n",
    "        projection = fluid.layers.elementwise_add(projection, self.softmax_bias)\n",
    "        projection = fluid.layers.reshape(\n",
    "            projection, shape=[-1, self.class_num])\n",
    "        pred = fluid.layers.softmax(projection, axis=-1)\n",
    "        \n",
    "        # 根据给定的标签信息，计算整个网络的损失函数，这里我们可以直接使用分类任务中常使用的交叉熵来训练网络\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(\n",
    "            logits=projection, label=label, soft_label=False)\n",
    "        loss = fluid.layers.reduce_mean(loss)\n",
    "\n",
    "        # 最终返回预测结果pred，和网络的loss\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10, loss 0.643\n",
      "step 20, loss 0.646\n",
      "step 30, loss 0.632\n",
      "step 40, loss 0.624\n",
      "step 50, loss 0.645\n",
      "step 60, loss 0.627\n",
      "step 70, loss 0.647\n",
      "step 80, loss 0.648\n",
      "step 90, loss 0.632\n",
      "step 100, loss 0.678\n",
      "step 110, loss 0.661\n",
      "step 120, loss 0.639\n",
      "step 130, loss 0.610\n",
      "step 140, loss 0.677\n",
      "step 150, loss 0.608\n",
      "step 160, loss 0.666\n",
      "step 170, loss 0.640\n",
      "step 180, loss 0.664\n",
      "step 190, loss 0.653\n",
      "step 200, loss 0.640\n",
      "step 210, loss 0.627\n",
      "step 220, loss 0.654\n",
      "step 230, loss 0.692\n",
      "step 240, loss 0.631\n",
      "step 250, loss 0.555\n",
      "step 260, loss 0.544\n",
      "step 270, loss 0.456\n",
      "step 280, loss 0.417\n",
      "step 290, loss 0.432\n",
      "step 300, loss 0.318\n",
      "step 310, loss 0.291\n",
      "step 320, loss 0.315\n",
      "step 330, loss 0.332\n",
      "step 340, loss 0.528\n",
      "step 350, loss 0.313\n",
      "step 360, loss 0.327\n",
      "step 370, loss 0.268\n",
      "the acc in the test set is 0.845\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "import paddle.fluid as fluid\n",
    "#开始训练\n",
    "batch_size = 128\n",
    "epoch_num = 5\n",
    "embedding_size = 256\n",
    "step = 0\n",
    "learning_rate = 0.01\n",
    "max_seq_len = 128\n",
    "\n",
    "# 使用GPU进行训练\n",
    "use_gpu=True\n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "with fluid.dygraph.guard(place):\n",
    "    # 创建一个用于情感分类的网络实例，sentiment_classifier\n",
    "    sentiment_classifier = SentimentClassifier(\n",
    "    \tembedding_size, vocab_size, num_steps=max_seq_len)\n",
    "    # 创建优化器AdamOptimizer，用于更新这个网络的参数\n",
    "    adam = fluid.optimizer.AdamOptimizer(learning_rate=learning_rate, parameter_list = sentiment_classifier.parameters())\n",
    "\n",
    "    for sentences, labels in build_batch(\n",
    "        word2id_dict, train_corpus, batch_size, epoch_num, max_seq_len):\n",
    "        \n",
    "        sentences_var = fluid.dygraph.to_variable(sentences)\n",
    "        labels_var = fluid.dygraph.to_variable(labels)\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "\n",
    "        loss.backward()\n",
    "        adam.minimize(loss)\n",
    "        sentiment_classifier.clear_gradients()\n",
    "        \n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "            \n",
    "    # 保存模型\n",
    "    fluid.save_dygraph(sentiment_classifier.state_dict(), 'sentiment')\n",
    "    \n",
    "    # 我们希望在网络训练结束以后评估一下训练好的网络的效果\n",
    "    # 通过eval()函数，将网络设置为eval模式，在eval模式中，网络不会进行梯度更新\n",
    "    sentiment_classifier.eval()\n",
    "    # 这里我们需要记录模型预测结果的准确率\n",
    "    # 对于二分类任务来说，准确率的计算公式为：\n",
    "    # (true_positive + true_negative) / \n",
    "    # (true_positive + true_negative + false_positive + false_negative)\n",
    "    tp = 0.\n",
    "    tn = 0.\n",
    "    fp = 0.\n",
    "    fn = 0.\n",
    "    for sentences, labels in build_batch(\n",
    "        word2id_dict, test_corpus, batch_size, 1, max_seq_len):\n",
    "        \n",
    "        sentences_var = fluid.dygraph.to_variable(sentences)\n",
    "        labels_var = fluid.dygraph.to_variable(labels)\n",
    "        \n",
    "        # 获取模型对当前batch的输出结果\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "\n",
    "        # 把输出结果转换为numpy array的数据结构\n",
    "        # 遍历这个数据结构，比较预测结果和对应label之间的关系，并更新tp，tn，fp和fn\n",
    "        pred = pred.numpy()\n",
    "        for i in range(len(pred)):\n",
    "            if labels[i][0] == 1:\n",
    "                if pred[i][1] > pred[i][0]:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            else:\n",
    "                if pred[i][1] > pred[i][0]:\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "\n",
    "    # 输出最终评估的模型效果\n",
    "    print(\"the acc in the test set is %.3f\" % ((tp + tn) / (tp + tn + fp + fn)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
